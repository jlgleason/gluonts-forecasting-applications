{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    ">1. fill 'NaN' Province/State values with Country/Region values\n",
    ">2. (optionally) apply log transformation to target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and clean data\n",
    "import pandas as pd\n",
    "\n",
    "train_all = pd.read_csv(\"/root/data/train.csv\")\n",
    "\n",
    "def preprocess(\n",
    "    frame: pd.DataFrame,\n",
    "    log_transform: bool = False\n",
    "):\n",
    "    \n",
    "    # set index\n",
    "    new_frame = frame.set_index('Date')\n",
    "\n",
    "    # fill 'NaN' Province/State values with Country/Region values\n",
    "    new_frame['Province/State'] = new_frame['Province/State'].fillna(new_frame['Country/Region'])\n",
    "    \n",
    "    # convert target values to log scale\n",
    "    if log_transform:\n",
    "        new_frame[['ConfirmedCases', 'Fatalities']] = np.log1p(\n",
    "            new_frame[['ConfirmedCases', 'Fatalities']].values\n",
    "    )\n",
    "    \n",
    "    return new_frame\n",
    "\n",
    "def split(\n",
    "    df: pd.DataFrame, \n",
    "    date: str = '2020-03-12'\n",
    "):\n",
    "    train = df.loc[train_all.index < date] \n",
    "    test = df.loc[train_all.index >= date]\n",
    "    return train, test\n",
    "\n",
    "train_all = preprocess(train_all)\n",
    "train, test = split(train_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confirmed cases and fatalities in train\n",
    "import matplotlib.pyplot as plt\n",
    "from gluonts.dataset.util import to_pandas\n",
    "from gluonts.dataset.common import ListDataset\n",
    "\n",
    "cum_train = train.groupby('Date').sum()\n",
    "cum_test = test.groupby('Date').sum()\n",
    "\n",
    "def plot_observations(\n",
    "    target: str = 'ConfirmedCases'\n",
    "):\n",
    "    fig = plt.figure(figsize=(15, 6.1), facecolor=\"white\",  edgecolor='k')\n",
    "\n",
    "    train_ds = ListDataset(\n",
    "        [{\"start\": cum_train.index[0], \"target\": cum_train[target].values}],\n",
    "        freq = \"D\",\n",
    "    )\n",
    "    test_ds = ListDataset(\n",
    "        [{\"start\": cum_test.index[0], \"target\": cum_test[target].values}],\n",
    "        freq = \"D\",\n",
    "    )\n",
    "    \n",
    "    for tr, te in zip(train_ds, test_ds):\n",
    "        tr = to_pandas(tr)\n",
    "        te = to_pandas(te)\n",
    "        tr.plot(linewidth=2, label = f'train {target}')\n",
    "        tr[-1:].append(te).plot(linewidth=2, label = f'test {target}')\n",
    "    \n",
    "    plt.axvline(cum_train.index[-1], color='purple') # end of train dataset\n",
    "    plt.title(f'Cumulative global number of {target}', fontsize=16)\n",
    "    plt.legend(fontsize=16)\n",
    "    plt.grid(which=\"both\")\n",
    "    plt.show()\n",
    "    \n",
    "plot_observations('ConfirmedCases')\n",
    "plot_observations('Fatalities')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation\n",
    "\n",
    ">1. Categorical feature for 'Province/State'\n",
    "2. Categorical feature for 'Country/Region'\n",
    "3. Augment with 2020 country level population data from https://www.kaggle.com/tanuprabhu/population-by-country-2020.  This approach was inspired by this previous submission to the competition: https://www.kaggle.com/saga21/covid-global-forecast-sir-model-ml-regressions:\n",
    "  1. Population\n",
    "  2. % Population Increase 2019-2020\n",
    "  3. Net Population Increase 2019-2020\n",
    "  4. Population Density (P/Km^2)\n",
    "  5. Land Area (Km^2)\n",
    "  6. Net Migration\n",
    "  7. Fertility Rate\n",
    "  8. Median Age\n",
    "  9. % Urban Population\n",
    "  10. % of World Population\n",
    "  \n",
    "**Current Status**: To our surprise, we actually found that the forecasts produced by the DeepAR model were worse with these country-level statistics as additional features. Adding these covariates seemed to bias all forecasts toward 0. We  hypothesize that this is because the distribution of cases/fatalities by province/state for a single country (which has a one-to-many mapping to provinces/states) is right skewed. Thus the model is rewarded for learning the low volume series and the learned parameters and resulting forecasts reflect this. Thus, in the forecasts we have submitted, these country-level statistics have not been included as additional features (see `use_real_vars` argument in `fit()` function defined in this notebook). \n",
    "\n",
    "*TODO*:\n",
    ">1. Experiment with other covariates (state level population data, healthcare infrastructure dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "pop_df = pd.read_csv('/root/data/population_by_country_2020.csv')\n",
    "\n",
    "country_map = {\n",
    "    \"Congo (Brazzaville)\": \"Congo\",\n",
    "    \"Congo (Kinshasa)\": \"Congo\",\n",
    "    \"Cote d'Ivoire\": \"Côte d'Ivoire\",\n",
    "    \"Czechia\": \"Czech Republic (Czechia)\",\n",
    "    \"Gambia, The\": \"Gambia\",\n",
    "    \"Guernsey\": \"Channel Islands\",\n",
    "    \"Jersey\": \"Channel Islands\",\n",
    "    \"Korea, South\": \"South Korea\",\n",
    "    \"Republic of the Congo\": \"DR Congo\",\n",
    "    \"Reunion\": \"Réunion\",\n",
    "    \"Saint Vincent and the Grenadines\": \"St. Vincent & Grenadines\",\n",
    "    \"Taiwan*\": \"Taiwan\",\n",
    "    \"The Bahamas\": \"Bahamas\",\n",
    "    \"The Gambia\": \"Gambia\",\n",
    "    \"US\": \"United States\"\n",
    "}\n",
    "\n",
    "def clean_pop_data(\n",
    "    pop_df: pd.DataFrame\n",
    "):\n",
    "    \"\"\" bootstrapped from https://www.kaggle.com/saga21/covid-global-forecast-sir-model-ml-regressions\"\"\"\n",
    "        \n",
    "    # remove the % character from Urban Pop, World Share, and Yearly Change values \n",
    "    pop_df['Yearly Change'] = pop_df['Yearly Change'].str.rstrip('%')\n",
    "    pop_df['Urban Pop %'] = pop_df['Urban Pop %'].str.rstrip('%')\n",
    "    pop_df['World Share'] = pop_df['World Share'].str.rstrip('%')\n",
    "    \n",
    "    # Replace Urban Pop, Fertility Rate, and Med Age \"N.A\" by their respective modes\n",
    "    pop_df.loc[pop_df['Urban Pop %']=='N.A.', 'Urban Pop %'] = pop_df.loc[pop_df['Urban Pop %']!='N.A.', 'Urban Pop %'].mode()[0]\n",
    "    pop_df.loc[pop_df['Med. Age']=='N.A.', 'Med. Age'] = pop_df.loc[pop_df['Med. Age']!='N.A.', 'Med. Age'].mode()[0]    \n",
    "    pop_df.loc[pop_df['Fert. Rate']=='N.A.', 'Fert. Rate'] = pop_df.loc[pop_df['Fert. Rate']!='N.A.', 'Fert. Rate'].mode()[0]    \n",
    "    \n",
    "    # replace empty migration rows with 0s\n",
    "    pop_df['Migrants (net)'] =  pop_df['Migrants (net)'].fillna(0)\n",
    "    \n",
    "    # cast to types\n",
    "    pop_df = pop_df.astype({\n",
    "        \"Population (2020)\": int,\n",
    "        \"Yearly Change\": float,\n",
    "        \"Net Change\": int,\n",
    "        \"Density (P/Km²)\": int,\n",
    "        \"Land Area (Km²)\": int,\n",
    "        \"Migrants (net)\": float,\n",
    "        \"Fert. Rate\": float,\n",
    "        \"Med. Age\": int,\n",
    "        \"Urban Pop %\": int,\n",
    "        \"World Share\": float,\n",
    "    })   \n",
    "\n",
    "    return pop_df\n",
    "\n",
    "def join(\n",
    "    df: pd.DataFrame,\n",
    "    pop_df: pd.DataFrame\n",
    "):\n",
    "    \n",
    "    # add join column with country mapping\n",
    "    df['join_col'] = [\n",
    "        val if val not in country_map.keys() \n",
    "        else country_map[val] \n",
    "        for val in df['Country/Region'].values\n",
    "    ]\n",
    "    \n",
    "    # join, delete merge columns\n",
    "    new_df = df.reset_index().merge(\n",
    "        pop_df,\n",
    "        left_on = 'join_col',\n",
    "        right_on = 'Country (or dependency)',\n",
    "        how = 'left'\n",
    "    ).set_index('Date')\n",
    "    new_df = new_df.drop(columns=['join_col', 'Country (or dependency)'])\n",
    "    \n",
    "    # replace columns that weren't matched in join with mean\n",
    "    new_df = new_df.fillna(new_df.mean())\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "def encode(\n",
    "    df: pd.DataFrame\n",
    "):\n",
    "    \"\"\" encode 'Province/State' and 'Country/Region' categorical variables as numerical ordinals\"\"\"\n",
    "    \n",
    "    enc = OrdinalEncoder()\n",
    "    df[['Province/State', 'Country/Region']] = enc.fit_transform(\n",
    "        df[['Province/State', 'Country/Region']].values\n",
    "    )\n",
    "    return df, enc\n",
    "\n",
    "pop_df = clean_pop_data(pop_df)\n",
    "join_df = join(train_all, pop_df)\n",
    "\n",
    "all_df, enc = encode(join_df)\n",
    "train_df, _ = split(all_df)\n",
    "_, val_df = split(all_df, date = '2020-02-28')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.dataset.common import ListDataset\n",
    "from gluonts.dataset.field_names import FieldName\n",
    "import typing\n",
    "\n",
    "REAL_VARS = [\n",
    "    'Population (2020)', \n",
    "    'Yearly Change', \n",
    "    'Net Change', \n",
    "    'Density (P/Km²)', \n",
    "    'Land Area (Km²)',\n",
    "    'Migrants (net)',\n",
    "    'Fert. Rate',\n",
    "    'Med. Age',\n",
    "    'Urban Pop %',\n",
    "    'World Share'\n",
    "]\n",
    "\n",
    "def build_dataset(\n",
    "    frame: pd.DataFrame,\n",
    "    target: str = 'Fatalities',\n",
    "    cat_vars: typing.List[str] = ['Province/State', 'Country/Region'],\n",
    "    real_vars: typing.List[str] = REAL_VARS\n",
    "):\n",
    "    return ListDataset(\n",
    "        [\n",
    "            {\n",
    "                FieldName.START: df.index[0], \n",
    "                FieldName.TARGET: df[target].values,\n",
    "                FieldName.FEAT_STATIC_CAT: df[cat_vars].values[0],\n",
    "                FieldName.FEAT_STATIC_REAL: df[real_vars].values[0]\n",
    "            }\n",
    "            for g, df in frame.groupby(by=['Province/State', 'Country/Region'])\n",
    "        ],\n",
    "        freq = \"D\",\n",
    "    )\n",
    "\n",
    "training_data_fatalities = build_dataset(train_df)\n",
    "training_data_cases = build_dataset(train_df, target = 'ConfirmedCases')\n",
    "training_data_fatalities_all = build_dataset(all_df)\n",
    "training_data_cases_all = build_dataset(all_df, target = 'ConfirmedCases')\n",
    "val_data_fatalities = build_dataset(val_df)\n",
    "val_data_cases = build_dataset(val_df, target = 'ConfirmedCases')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit DeepAR Model Estimates\n",
    "\n",
    "The DeepAR model was proposed by David Salinas, Valentin Flunkert, and Jan Gasthaus in \"DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks\" (https://arxiv.org/abs/1704.04110). The approach trains an autoregressive RNN to produces time-variant parameters of a specified distribution on a large collection of related time series. The learned distribution can then be used to produce probabilistic forecasts. Here we use the authors' *GluonTS* implementation (https://gluon-ts.mxnet.io/index.html).\n",
    "\n",
    "We believe the probabilistic nature of the DeepAR forecasts is a feature that differentiates our approach from others we have seen so far. Specifically, the ability to provide both confidence intervals and point estimates allows one to better understand the range of possible trajectories, from the worst-case scenario, to the best-case scenario, to the expected scenario. \n",
    "\n",
    "*TODO:*\n",
    "> 1. Experiment with adding learned Box Cox transformation before learning Negative Binomial paramaters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gluonts.model.deepar import DeepAREstimator\n",
    "from gluonts.trainer import Trainer\n",
    "from gluonts.distribution import NegativeBinomialOutput\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "\n",
    "# set random seeds for reproducibility\n",
    "mx.random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "def fit(\n",
    "    training_data: ListDataset,\n",
    "    validation_data: ListDataset = None,\n",
    "    use_real_vars: bool = False,\n",
    "    pred_length: int = 14,\n",
    "):\n",
    "    estimator = DeepAREstimator(\n",
    "        freq=\"D\", \n",
    "        prediction_length=pred_length,\n",
    "        use_feat_static_real = use_real_vars,\n",
    "        use_feat_static_cat = True,\n",
    "        cardinality = [train['Province/State'].nunique(), train['Country/Region'].nunique()],\n",
    "        distr_output=NegativeBinomialOutput(),\n",
    "        trainer=Trainer(\n",
    "            epochs=20,\n",
    "            learning_rate=0.005, \n",
    "            batch_size=64,\n",
    "            weight_decay=5e-5,\n",
    "            learning_rate_decay_factor=0.1,\n",
    "            patience=15,\n",
    "        ),\n",
    "    )\n",
    "    _, trained_net, predictor = estimator.train_model(\n",
    "        training_data = training_data, \n",
    "        validation_data = validation_data\n",
    "    )\n",
    "    \n",
    "    return predictor, trained_net\n",
    "\n",
    "#predictor_fatalities, train_fatality_net = fit(training_data_fatalities, val_data_fatalities)\n",
    "#predictor_cases, train_case_net = fit(training_data_cases, val_data_cases)\n",
    "predictor_fatalities_all, all_fatality_net = fit(training_data_fatalities_all, pred_length = 30)\n",
    "#predictor_cases_all, all_case_net = fit(training_data_cases_all, pred_length = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot predictions from fit model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.dataset.util import to_pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "\n",
    "def plot_forecast(\n",
    "    predictor,\n",
    "    location: List[str] = ['Italy', 'Italy'],\n",
    "    target: str = 'Fatalities',\n",
    "    cat_vars: typing.List[str] = ['Province/State', 'Country/Region'],\n",
    "    real_vars: typing.List[str] = REAL_VARS,\n",
    "    log_preds: bool = False,\n",
    "    fontsize: int = 16\n",
    "):\n",
    "    fig = plt.figure(figsize=(15, 6.1), facecolor=\"white\",  edgecolor='k')\n",
    "\n",
    "    # plot train observations, true observations from public test set, and forecasts\n",
    "    location_tr = enc.transform(np.array(location).reshape(1,-1))\n",
    "    tr_df = train_df[np.all((train_df[['Province/State', 'Country/Region']].values == location_tr), axis=1)]\n",
    "    train_obs = ListDataset(\n",
    "        [{\n",
    "            FieldName.START: tr_df.index[0], \n",
    "            FieldName.TARGET: tr_df[target].values,\n",
    "            FieldName.FEAT_STATIC_CAT: tr_df[cat_vars].values[0],\n",
    "            FieldName.FEAT_STATIC_REAL: tr_df[real_vars].values[0]\n",
    "        }],\n",
    "        freq = \"D\",\n",
    "    )\n",
    "    te_df = test_df[np.all((test_df[['Province/State', 'Country/Region']].values == location_tr), axis=1)]\n",
    "    test_gt = ListDataset(\n",
    "        [{\"start\": te_df.index[0], \"target\": te_df[target].values}],\n",
    "        freq = \"D\",\n",
    "    )\n",
    "    for train_series, gt, forecast in zip(train_obs, test_gt, predictor.predict(train_obs)):\n",
    "        \n",
    "        train_series = to_pandas(train_series)\n",
    "        gt = to_pandas(gt)\n",
    "        \n",
    "        if log_preds:\n",
    "            train_series = np.expm1(train_series)\n",
    "            gt = np.expm1(gt)\n",
    "            forecast.samples = np.expm1(forecast.samples)\n",
    "        \n",
    "        train_series.plot(linewidth=2, label = 'train series')\n",
    "        gt.plot(linewidth=2, label = 'test ground truth')\n",
    "        forecast.plot(color='g', prediction_intervals=[50.0, 90.0])\n",
    "        \n",
    "    plt.title(f'Cumulative number of {target} in {location[0]}', fontsize=fontsize)\n",
    "    plt.legend(fontsize = fontsize)\n",
    "    plt.grid(which='both')\n",
    "    plt.show()\n",
    "    \n",
    "plot_forecast(predictor_fatalities, ['Italy', 'Italy'])\n",
    "plot_forecast(predictor_fatalities, ['California', 'US'])\n",
    "plot_forecast(predictor_fatalities, ['Korea, South', 'Korea, South'])\n",
    "plot_forecast(predictor_fatalities, ['Hubei', 'China'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate metrics on public test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "from gluonts.evaluation import Evaluator\n",
    "import json\n",
    "\n",
    "all_data = build_dataset(all_df)\n",
    "\n",
    "# evaluate fatalities predictor\n",
    "forecast_iterable, ts_iterable = make_evaluation_predictions(\n",
    "    dataset=all_data,\n",
    "    predictor=predictor_fatalities,\n",
    "    num_samples=100\n",
    ")\n",
    "evaluator = Evaluator(quantiles=[0.1, 0.5, 0.9])\n",
    "agg_metrics, item_metrics = evaluator(ts_iterable, forecast_iterable, num_series=len(all_data))\n",
    "print('Fatalities Predictor Metrics: ')\n",
    "print(json.dumps(agg_metrics, indent=4))\n",
    "\n",
    "# evaluate confirmed cases predictor\n",
    "forecast_iterable, ts_iterable = make_evaluation_predictions(\n",
    "    dataset=all_data,\n",
    "    predictor=predictor_cases,\n",
    "    num_samples=100\n",
    ")\n",
    "agg_metrics, item_metrics = evaluator(ts_iterable, forecast_iterable, num_series=len(all_data))\n",
    "print('Confirmed Cases Predictor Metrics: ')\n",
    "print(json.dumps(agg_metrics, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate submission csv\n",
    "\n",
    "# aggregate fatalities\n",
    "fatalities = []\n",
    "for public_forecast, private_forecast in zip(\n",
    "    predictor_fatalities.predict(training_data_fatalities),\n",
    "    predictor_fatalities_all.predict(training_data_fatalities_all)\n",
    "):\n",
    "    # offset by 1 because last training date is March 24, want to start predicting at March 26\n",
    "    fatalities.append(np.concatenate((public_forecast.median, private_forecast.median[1:])))\n",
    "\n",
    "# aggregate cases\n",
    "cases = []\n",
    "for public_forecast, private_forecast in zip(\n",
    "    predictor_cases.predict(training_data_cases),\n",
    "    predictor_cases_all.predict(training_data_cases_all)\n",
    "):\n",
    "    # offset by 1 because last training date is March 24, want to start predicting at March 26\n",
    "    cases.append(np.concatenate((public_forecast.median, private_forecast.median[1:])))\n",
    "\n",
    "# load test csv \n",
    "sub_df = pd.read_csv(\"/root/data/test.csv\")\n",
    "\n",
    "# fill 'NaN' Province/State values with Country/Region values\n",
    "sub_df['Province/State'] = sub_df['Province/State'].fillna(sub_df['Country/Region'])\n",
    "\n",
    "# get forecast ids\n",
    "ids = []\n",
    "for _, df in sub_df.groupby(by=['Province/State', 'Country/Region']):\n",
    "    ids.append(df['ForecastId'].values)\n",
    "\n",
    "# create submission df\n",
    "submission = pd.DataFrame(\n",
    "    list(zip(\n",
    "        np.array(ids).flatten(),\n",
    "        np.array(cases).flatten(),\n",
    "        np.array(fatalities).flatten()\n",
    "    )), \n",
    "    columns = ['ForecastId', 'ConfirmedCases', 'Fatalities']\n",
    ")\n",
    "submission.to_csv('submission_dum.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Interrogate importance of covariates \n",
    "\n",
    "how - maybe model-agnostic method like SHAP values?, visualization of learned embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import manifold\n",
    "from matplotlib.ticker import NullFormatter\n",
    "\n",
    "# get list of countries with most fatalities\n",
    "def visualize_embedding(\n",
    "    trained_net,\n",
    "    sort_key: str = 'Fatalities',\n",
    "    top_n: int = 10\n",
    "):\n",
    "    countries = train_all.groupby('Country/Region').sum().sort_values(\n",
    "        by=sort_key, \n",
    "        ascending = False\n",
    "    ).head(top_n).index.tolist()\n",
    "\n",
    "    # visualize 2-D projection of learned Country/Region embedding space with TSNE\n",
    "    embedding = trained_net.collect_params()[f'{trained_net.name}_featureembedder0_cat_1_embedding_weight'].data()\n",
    "    proj = manifold.TSNE(init='pca', random_state = 0).fit_transform(embedding.asnumpy())\n",
    "\n",
    "    # plot\n",
    "    fig = plt.figure(figsize=(15, 6.1), facecolor=\"white\",  edgecolor='k')\n",
    "    ax = plt.gca()\n",
    "    for country in countries:\n",
    "        idx = np.where(enc.categories_[1] == country)[0][0]\n",
    "        plt.scatter(\n",
    "            proj[idx, 0], \n",
    "            proj[idx, 1], \n",
    "            cmap=plt.cm.Spectral, \n",
    "            label = country\n",
    "        )\n",
    "        ax.annotate(\n",
    "            country, \n",
    "            (proj[idx, 0], proj[idx, 1]), \n",
    "            fontsize=16,\n",
    "        )\n",
    "    ax.xaxis.set_major_formatter(NullFormatter())\n",
    "    ax.yaxis.set_major_formatter(NullFormatter())\n",
    "    plt.title(f\"TSNE Visualization of Learned 'Country/Region' Embedding Space\", fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "visualize_embedding(trained_case_net, sort_key = 'ConfirmedCases')\n",
    "visualize_embedding(all_fatality_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
